{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from scipy import stats\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read model training results\n",
    "cv_results = pickle.load(open('cv_results_3.p', 'rb'))\n",
    "results_final = np.load('results_final_3.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use results_final to construct tables ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 7, 3, 20)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 7, 3, 5, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reshape results_final to make \"trials\" a separate dimension\n",
    "## Resulting dimensions:\n",
    "##  1D -> 2 {train = 0, test = 1}\n",
    "##  2D -> 7 models\n",
    "##  3D -> 3 metrics\n",
    "##  4D -> 5 trials\n",
    "##  5D -> 4 datasets\n",
    "\n",
    "results_final_dataset_1 = results_final[:, :, :, 0:5].reshape(2, 7, 3, 5, 1)\n",
    "results_final_dataset_2 = results_final[:, :, :, 5:10].reshape(2, 7, 3, 5, 1)\n",
    "results_final_dataset_3 = results_final[:, :, :, 10:15].reshape(2, 7, 3, 5, 1)\n",
    "results_final_dataset_4 = results_final[:, :, :, 15:20].reshape(2, 7, 3, 5, 1)\n",
    "results_final = np.concatenate((results_final_dataset_1, results_final_dataset_2, results_final_dataset_3, results_final_dataset_4), axis=4)\n",
    "results_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Average results_final across trials to get a 2x7x3x4 numpy array\n",
    "results_final_trial_avg = np.mean(results_final, axis = 3)\n",
    "results_final_trial_avg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Table 2: mean test set performance across trials for each algorithm/dataset combo\n",
    "##   shape = 7x(3+1) last column is the means\n",
    "##   rows -> models\n",
    "##   cols -> metrics\n",
    "table_2 = np.mean(results_final_trial_avg[1, :, :, :], axis=2)\n",
    "table_2_mean = np.mean(table_2, axis=1).reshape(7, 1)  ## Mean across metrics\n",
    "table_2 = np.concatenate((table_2, table_2_mean), axis=1)\n",
    "\n",
    "print(np.round(table_2, decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Table 3: mean test set performance across trials for each algorithm/metric combo\n",
    "##   shape = 7x(4+1) last column is the means\n",
    "##   rows -> models\n",
    "##   cols -> datasets\n",
    "table_3 = np.mean(results_final_trial_avg[1, :, :, :], axis=1)\n",
    "table_3_mean = np.mean(table_3, axis=1).reshape(7, 1)  ## Mean across datasets\n",
    "table_3 = np.concatenate((table_3, table_3_mean), axis=1)\n",
    "\n",
    "print(np.round(table_3, decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Table 4 (Appendix): mean training set performance across trials for each algorithm/dataset combo\n",
    "##   shape = 7x(3+1) last column is the means\n",
    "##   rows -> models\n",
    "##   cols -> metrics\n",
    "table_4 = np.mean(results_final_trial_avg[0, :, :, :], axis=2)\n",
    "table_4_mean = np.mean(table_4, axis=1).reshape(7, 1)  ## Mean across metrics\n",
    "table_4 = np.concatenate((table_4, table_4_mean), axis=1)\n",
    "\n",
    "print(np.round(table_4, decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Table 5 (Appendix): raw testing set performance\n",
    "##   shape = 7x3x4 \n",
    "##   rows (1D) -> models\n",
    "##   cols (2D) -> metrics\n",
    "##        (3D) -> datasets\n",
    "table_5 = results_final_trial_avg[1, :, :, :]\n",
    "\n",
    "print(np.round(table_5[:, :, 0], decimals=3))  ## Adult dataset\n",
    "print(np.round(table_5[:, :, 1], decimals=3))  ## Occupancy dataset\n",
    "print(np.round(table_5[:, :, 2], decimals=3))  ## HTRU2 dataset\n",
    "print(np.round(table_5[:, :, 3], decimals=3))  ## Activity dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.829 0.598 0.726]\n",
      " [0.649 0.602 0.745]\n",
      " [0.828 0.595 0.724]\n",
      " [0.828 0.601 0.729]\n",
      " [0.797 0.564 0.708]\n",
      " [0.812 0.568 0.711]\n",
      " [0.828 0.601 0.73 ]]\n",
      "[[0.828 0.602 0.73 ]\n",
      " [0.787 0.531 0.693]\n",
      " [0.829 0.612 0.74 ]\n",
      " [0.828 0.61  0.736]\n",
      " [0.803 0.578 0.724]\n",
      " [0.809 0.577 0.718]\n",
      " [0.828 0.594 0.723]]\n",
      "[[0.828 0.602 0.729]\n",
      " [0.771 0.412 0.62 ]\n",
      " [0.826 0.571 0.711]\n",
      " [0.826 0.599 0.727]\n",
      " [0.799 0.564 0.7  ]\n",
      " [0.813 0.58  0.715]\n",
      " [0.829 0.574 0.722]]\n",
      "[[0.829 0.605 0.731]\n",
      " [0.766 0.585 0.735]\n",
      " [0.822 0.587 0.721]\n",
      " [0.826 0.6   0.729]\n",
      " [0.799 0.547 0.716]\n",
      " [0.814 0.573 0.712]\n",
      " [0.83  0.597 0.729]]\n",
      "[[0.826 0.591 0.722]\n",
      " [0.792 0.414 0.627]\n",
      " [0.826 0.564 0.705]\n",
      " [0.827 0.566 0.707]\n",
      " [0.796 0.579 0.724]\n",
      " [0.812 0.572 0.714]\n",
      " [0.826 0.579 0.717]]\n"
     ]
    }
   ],
   "source": [
    "## Table 5 (Appendix): raw testing set performance\n",
    "##   Column 1: Adult dataset\n",
    "print(np.round(results_final[1, :, :, 0, 0], decimals=3))  ## Adult dataset trial 1\n",
    "print(np.round(results_final[1, :, :, 1, 0], decimals=3))  ## Adult dataset trial 2\n",
    "print(np.round(results_final[1, :, :, 2, 0], decimals=3))  ## Adult dataset trial 3\n",
    "print(np.round(results_final[1, :, :, 3, 0], decimals=3))  ## Adult dataset trial 4\n",
    "print(np.round(results_final[1, :, :, 4, 0], decimals=3))  ## Adult dataset trial 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.988 0.976 0.991]\n",
      " [0.985 0.968 0.989]\n",
      " [0.988 0.975 0.991]\n",
      " [0.99  0.978 0.993]\n",
      " [0.989 0.976 0.991]\n",
      " [0.991 0.981 0.991]\n",
      " [0.99  0.979 0.992]]\n",
      "[[0.989 0.977 0.992]\n",
      " [0.988 0.975 0.988]\n",
      " [0.989 0.976 0.991]\n",
      " [0.99  0.979 0.992]\n",
      " [0.989 0.977 0.991]\n",
      " [0.99  0.98  0.99 ]\n",
      " [0.989 0.977 0.987]]\n",
      "[[0.988 0.974 0.99 ]\n",
      " [0.913 0.777 0.822]\n",
      " [0.988 0.975 0.99 ]\n",
      " [0.988 0.974 0.992]\n",
      " [0.988 0.975 0.99 ]\n",
      " [0.991 0.981 0.99 ]\n",
      " [0.99  0.979 0.99 ]]\n",
      "[[0.989 0.976 0.991]\n",
      " [0.902 0.825 0.936]\n",
      " [0.988 0.975 0.991]\n",
      " [0.988 0.976 0.991]\n",
      " [0.989 0.975 0.991]\n",
      " [0.992 0.981 0.991]\n",
      " [0.991 0.981 0.991]]\n",
      "[[0.99  0.978 0.992]\n",
      " [0.978 0.894 0.985]\n",
      " [0.989 0.976 0.991]\n",
      " [0.991 0.98  0.993]\n",
      " [0.99  0.978 0.991]\n",
      " [0.992 0.982 0.991]\n",
      " [0.991 0.981 0.991]]\n"
     ]
    }
   ],
   "source": [
    "##   Column 2: Occupancy dataset\n",
    "print(np.round(results_final[1, :, :, 0, 1], decimals=3))  ## Occupancy dataset trial 1\n",
    "print(np.round(results_final[1, :, :, 1, 1], decimals=3))  ## Occupancy dataset trial 2\n",
    "print(np.round(results_final[1, :, :, 2, 1], decimals=3))  ## Occupancy dataset trial 3\n",
    "print(np.round(results_final[1, :, :, 3, 1], decimals=3))  ## Occupancy dataset trial 4\n",
    "print(np.round(results_final[1, :, :, 4, 1], decimals=3))  ## Occupancy dataset trial 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98  0.882 0.912]\n",
      " [0.976 0.855 0.891]\n",
      " [0.979 0.876 0.907]\n",
      " [0.98  0.886 0.915]\n",
      " [0.978 0.874 0.91 ]\n",
      " [0.98  0.886 0.922]\n",
      " [0.98  0.885 0.923]]\n",
      "[[0.98  0.885 0.916]\n",
      " [0.977 0.868 0.762]\n",
      " [0.98  0.888 0.918]\n",
      " [0.979 0.877 0.91 ]\n",
      " [0.977 0.87  0.906]\n",
      " [0.98  0.882 0.918]\n",
      " [0.98  0.886 0.923]]\n",
      "[[0.98  0.882 0.914]\n",
      " [0.975 0.852 0.856]\n",
      " [0.98  0.883 0.915]\n",
      " [0.979 0.875 0.906]\n",
      " [0.977 0.869 0.911]\n",
      " [0.979 0.88  0.91 ]\n",
      " [0.98  0.883 0.917]]\n",
      "[[0.98  0.883 0.912]\n",
      " [0.947 0.598 0.714]\n",
      " [0.98  0.88  0.908]\n",
      " [0.98  0.891 0.917]\n",
      " [0.978 0.873 0.916]\n",
      " [0.98  0.886 0.917]\n",
      " [0.979 0.879 0.912]]\n",
      "[[0.98  0.883 0.914]\n",
      " [0.967 0.783 0.826]\n",
      " [0.98  0.882 0.912]\n",
      " [0.98  0.882 0.916]\n",
      " [0.978 0.873 0.916]\n",
      " [0.979 0.883 0.92 ]\n",
      " [0.98  0.886 0.926]]\n"
     ]
    }
   ],
   "source": [
    "##   Column 3: HTRU2 dataset\n",
    "print(np.round(results_final[1, :, :, 0, 2], decimals=3))  ## HTRU2 dataset trial 1\n",
    "print(np.round(results_final[1, :, :, 1, 2], decimals=3))  ## HTRU2 dataset trial 2\n",
    "print(np.round(results_final[1, :, :, 2, 2], decimals=3))  ## HTRU2 dataset trial 3\n",
    "print(np.round(results_final[1, :, :, 3, 2], decimals=3))  ## HTRU2 dataset trial 4\n",
    "print(np.round(results_final[1, :, :, 4, 2], decimals=3))  ## HTRU2 dataset trial 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.976 0.554 0.698]\n",
      " [0.885 0.025 0.506]\n",
      " [0.979 0.618 0.726]\n",
      " [0.98  0.667 0.784]\n",
      " [0.979 0.666 0.826]\n",
      " [0.983 0.731 0.801]\n",
      " [0.983 0.744 0.824]]\n",
      "[[0.977 0.571 0.706]\n",
      " [0.963 0.058 0.515]\n",
      " [0.979 0.624 0.729]\n",
      " [0.979 0.683 0.79 ]\n",
      " [0.978 0.675 0.852]\n",
      " [0.984 0.771 0.829]\n",
      " [0.985 0.787 0.854]]\n",
      "[[0.978 0.582 0.71 ]\n",
      " [0.963 0.353 0.69 ]\n",
      " [0.98  0.628 0.732]\n",
      " [0.98  0.662 0.782]\n",
      " [0.981 0.706 0.833]\n",
      " [0.986 0.771 0.822]\n",
      " [0.984 0.762 0.847]]\n",
      "[[0.977 0.572 0.706]\n",
      " [0.97  0.13  0.736]\n",
      " [0.979 0.623 0.728]\n",
      " [0.979 0.677 0.801]\n",
      " [0.981 0.705 0.848]\n",
      " [0.986 0.774 0.81 ]\n",
      " [0.985 0.763 0.841]]\n",
      "[[0.977 0.563 0.699]\n",
      " [0.967 0.018 0.5  ]\n",
      " [0.979 0.621 0.727]\n",
      " [0.978 0.657 0.801]\n",
      " [0.977 0.637 0.834]\n",
      " [0.983 0.747 0.821]\n",
      " [0.982 0.722 0.826]]\n"
     ]
    }
   ],
   "source": [
    "##   Column 4: Activity dataset\n",
    "print(np.round(results_final[1, :, :, 0, 3], decimals=3))  ## Activity dataset trial 1\n",
    "print(np.round(results_final[1, :, :, 1, 3], decimals=3))  ## Activity dataset trial 2\n",
    "print(np.round(results_final[1, :, :, 2, 3], decimals=3))  ## Activity dataset trial 3\n",
    "print(np.round(results_final[1, :, :, 3, 3], decimals=3))  ## Activity dataset trial 4\n",
    "print(np.round(results_final[1, :, :, 4, 3], decimals=3))  ## Activity dataset trial 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Table 6 (Appendix): pair-wise t-test for table 2 across 5 trials\n",
    "##   shape = 7x3\n",
    "##   rows -> models\n",
    "##   cols -> metrics\n",
    "\n",
    "table_6 = np.zeros((7, 3))\n",
    "table_6_prep = np.mean(results_final[1, :, :, :, :], axis=3)\n",
    "\n",
    "for i in range(3):  ## Loop through metrics\n",
    "    \n",
    "    idx_best = np.argmax(table_2[:, i])\n",
    "    \n",
    "    for j in range(7):  ## Loop through models\n",
    "    \n",
    "        p_val = stats.ttest_rel(table_6_prep[j, i, :], table_6_prep[idx_best, i, :])[1]\n",
    "        table_6[j, i] = p_val\n",
    "\n",
    "print(np.round(table_6, decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check which values we need to *\n",
    "table_6 > 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Table 7 (Appendix): pair-wise t-test for table 3 across 5 trials\n",
    "##   shape = 7x4\n",
    "##   rows -> models\n",
    "##   cols -> datasets\n",
    "\n",
    "table_7 = np.zeros((7, 4))\n",
    "table_7_prep = np.mean(results_final[1, :, :, :, :], axis=1)\n",
    "\n",
    "for i in range(4):  ## Loop through datasets\n",
    "    \n",
    "    idx_best = np.argmax(table_3[:, i])\n",
    "    \n",
    "    for j in range(7):  ## Loop through models\n",
    "    \n",
    "        p_val = stats.ttest_rel(table_7_prep[j, :, i], table_7_prep[idx_best, :, i])[1]\n",
    "        table_7[j, i] = p_val\n",
    "\n",
    "print(np.round(table_7, decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check which values we need to *\n",
    "table_7 > 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use cv_results to draw heatmaps ###\n",
    "420 = 20 x 7 (models) x 3 (metrics)<br>\n",
    "20 = 4 (datasets) x 5 (trials)<br>\n",
    "\n",
    "0 to 104:   Adult dataset<br>\n",
    "105 to 209:  Occupancy dataset<br>\n",
    "210 to 314:  HTRU2 dataset<br>\n",
    "315 to 419:  Activity dataset\n",
    "\n",
    "Same model and same metric for every 21 cv_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cv_results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_acc_idx =    list(np.arange(0, 420, 21))\n",
    "logreg_f1_idx =     list(np.arange(1, 420, 21))\n",
    "logreg_rocauc_idx = list(np.arange(2, 420, 21))\n",
    "\n",
    "percep_acc_idx =    list(np.arange(3, 420, 21))\n",
    "percep_f1_idx =     list(np.arange(4, 420, 21))\n",
    "percep_rocauc_idx = list(np.arange(5, 420, 21))\n",
    "\n",
    "linsvm_acc_idx =    list(np.arange(6, 420, 21))\n",
    "linsvm_f1_idx =     list(np.arange(7, 420, 21))\n",
    "linsvm_rocauc_idx = list(np.arange(8, 420, 21))\n",
    "\n",
    "rbfsvm_acc_idx =    list(np.arange(9, 420, 21))\n",
    "rbfsvm_f1_idx =     list(np.arange(10, 420, 21))\n",
    "rbfsvm_rocauc_idx = list(np.arange(11, 420, 21))\n",
    "\n",
    "dectre_acc_idx =    list(np.arange(12, 420, 21))\n",
    "dectre_f1_idx =     list(np.arange(13, 420, 21))\n",
    "dectre_rocauc_idx = list(np.arange(14, 420, 21))\n",
    "\n",
    "ranfor_acc_idx =    list(np.arange(15, 420, 21))\n",
    "ranfor_f1_idx =     list(np.arange(16, 420, 21))\n",
    "ranfor_rocauc_idx = list(np.arange(17, 420, 21))\n",
    "\n",
    "graboo_acc_idx =    list(np.arange(18, 420, 21))\n",
    "graboo_f1_idx =     list(np.arange(19, 420, 21))\n",
    "graboo_rocauc_idx = list(np.arange(20, 420, 21))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy\n",
    "param_search_results_logreg_acc = pd.DataFrame()\n",
    "\n",
    "for i in logreg_acc_idx:\n",
    "    param_search_results = pd.DataFrame(cv_results[i]['params'])\n",
    "    param_search_results['score_acc'] = cv_results[i]['mean_test_score']\n",
    "    param_search_results_logreg_acc = pd.concat([param_search_results_logreg_acc, param_search_results], ignore_index=True)\n",
    "\n",
    "param_search_results_logreg_acc = param_search_results_logreg_acc.groupby(['C'], as_index=True).mean()\n",
    "sns.heatmap(param_search_results_logreg_acc, annot=True, fmt='.4f')\n",
    "plt.title('Logistic Regression Accuracy Score')\n",
    "plt.savefig('results/param_search_results_logreg_acc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## F1\n",
    "param_search_results_logreg_f1 = pd.DataFrame()\n",
    "\n",
    "for i in logreg_f1_idx:\n",
    "    param_search_results = pd.DataFrame(cv_results[i]['params'])\n",
    "    param_search_results['score_f1'] = cv_results[i]['mean_test_score']\n",
    "    param_search_results_logreg_f1 = pd.concat([param_search_results_logreg_f1, param_search_results], ignore_index=True)\n",
    "\n",
    "param_search_results_logreg_f1 = param_search_results_logreg_f1.groupby(['C'], as_index=True).mean()\n",
    "sns.heatmap(param_search_results_logreg_f1, annot=True, fmt='.4f')\n",
    "plt.title('Logistic Regression F1 Score')\n",
    "plt.savefig('results/param_search_results_logreg_f1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROC AUC\n",
    "param_search_results_logreg_rocauc = pd.DataFrame()\n",
    "\n",
    "for i in logreg_rocauc_idx:\n",
    "    param_search_results = pd.DataFrame(cv_results[i]['params'])\n",
    "    param_search_results['score_rocauc'] = cv_results[i]['mean_test_score']\n",
    "    param_search_results_logreg_rocauc = pd.concat([param_search_results_logreg_rocauc, param_search_results], ignore_index=True)\n",
    "\n",
    "param_search_results_logreg_rocauc = param_search_results_logreg_rocauc.groupby(['C'], as_index=True).mean()\n",
    "sns.heatmap(param_search_results_logreg_rocauc, annot=True, fmt='.4f')\n",
    "plt.title('Logistic Regression ROC AUC Score')\n",
    "plt.savefig('results/param_search_results_logreg_rocauc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy\n",
    "param_search_results_percep_acc = pd.DataFrame()\n",
    "\n",
    "for i in percep_acc_idx:\n",
    "    param_search_results = pd.DataFrame(cv_results[i]['params'])\n",
    "    param_search_results['score_acc'] = cv_results[i]['mean_test_score']\n",
    "    param_search_results_percep_acc = pd.concat([param_search_results_percep_acc, param_search_results], ignore_index=True)\n",
    "\n",
    "param_search_results_percep_acc = param_search_results_percep_acc.groupby(['penalty', 'alpha'], as_index=True, group_keys=False).mean()\n",
    "param_search_results_percep_acc.reset_index(inplace=True)\n",
    "sns.heatmap(param_search_results_percep_acc.pivot('alpha', 'penalty', 'score_acc'), annot=True, fmt='.4f')\n",
    "plt.title('Perceptron Accuracy Score')\n",
    "plt.xlabel('penalty')\n",
    "plt.savefig('results/param_search_results_percep_acc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## F1\n",
    "param_search_results_percep_f1 = pd.DataFrame()\n",
    "\n",
    "for i in percep_f1_idx:\n",
    "    param_search_results = pd.DataFrame(cv_results[i]['params'])\n",
    "    param_search_results['score_f1'] = cv_results[i]['mean_test_score']\n",
    "    param_search_results_percep_f1 = pd.concat([param_search_results_percep_f1, param_search_results], ignore_index=True)\n",
    "\n",
    "param_search_results_percep_f1 = param_search_results_percep_f1.groupby(['penalty', 'alpha'], as_index=True, group_keys=False).mean()\n",
    "param_search_results_percep_f1.reset_index(inplace=True)\n",
    "sns.heatmap(param_search_results_percep_f1.pivot('alpha', 'penalty', 'score_f1'), annot=True, fmt='.4f')\n",
    "plt.title('Perceptron F1 Score')\n",
    "plt.xlabel('penalty')\n",
    "plt.savefig('results/param_search_results_percep_f1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROC AUC\n",
    "param_search_results_percep_rocauc = pd.DataFrame()\n",
    "\n",
    "for i in percep_rocauc_idx:\n",
    "    param_search_results = pd.DataFrame(cv_results[i]['params'])\n",
    "    param_search_results['score_rocauc'] = cv_results[i]['mean_test_score']\n",
    "    param_search_results_percep_rocauc = pd.concat([param_search_results_percep_rocauc, param_search_results], ignore_index=True)\n",
    "\n",
    "param_search_results_percep_rocauc = param_search_results_percep_rocauc.groupby(['penalty', 'alpha'], as_index=True, group_keys=False).mean()\n",
    "param_search_results_percep_rocauc.reset_index(inplace=True)\n",
    "sns.heatmap(param_search_results_percep_rocauc.pivot('alpha', 'penalty', 'score_rocauc'), annot=True, fmt='.4f')\n",
    "plt.title('Perceptron ROC AUC Score')\n",
    "plt.xlabel('penalty')\n",
    "plt.savefig('results/param_search_results_percep_rocauc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy\n",
    "param_search_results_linsvm_acc = pd.DataFrame()\n",
    "\n",
    "for i in linsvm_acc_idx:\n",
    "    param_search_results = pd.DataFrame(cv_results[i]['params'])\n",
    "    param_search_results['score_acc'] = cv_results[i]['mean_test_score']\n",
    "    param_search_results_linsvm_acc = pd.concat([param_search_results_linsvm_acc, param_search_results], ignore_index=True)\n",
    "\n",
    "param_search_results_linsvm_acc = param_search_results_linsvm_acc.groupby(['C'], as_index=True).mean()\n",
    "sns.heatmap(param_search_results_linsvm_acc, annot=True, fmt='.4f')\n",
    "plt.title('Linear SVM Accuracy Score')\n",
    "plt.savefig('results/param_search_results_linsvm_acc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## F1\n",
    "param_search_results_linsvm_f1 = pd.DataFrame()\n",
    "\n",
    "for i in linsvm_f1_idx:\n",
    "    param_search_results = pd.DataFrame(cv_results[i]['params'])\n",
    "    param_search_results['score_f1'] = cv_results[i]['mean_test_score']\n",
    "    param_search_results_linsvm_f1 = pd.concat([param_search_results_linsvm_f1, param_search_results], ignore_index=True)\n",
    "\n",
    "param_search_results_linsvm_f1 = param_search_results_linsvm_f1.groupby(['C'], as_index=True).mean()\n",
    "sns.heatmap(param_search_results_linsvm_f1, annot=True, fmt='.4f')\n",
    "plt.title('Linear SVM F1 Score')\n",
    "plt.savefig('results/param_search_results_linsvm_f1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROC AUC\n",
    "param_search_results_linsvm_rocauc = pd.DataFrame()\n",
    "\n",
    "for i in linsvm_rocauc_idx:\n",
    "    param_search_results = pd.DataFrame(cv_results[i]['params'])\n",
    "    param_search_results['score_rocauc'] = cv_results[i]['mean_test_score']\n",
    "    param_search_results_linsvm_rocauc = pd.concat([param_search_results_linsvm_rocauc, param_search_results], ignore_index=True)\n",
    "\n",
    "param_search_results_linsvm_rocauc = param_search_results_linsvm_rocauc.groupby(['C'], as_index=True).mean()\n",
    "sns.heatmap(param_search_results_linsvm_rocauc, annot=True, fmt='.4f')\n",
    "plt.title('Linear SVM ROC AUC Score')\n",
    "plt.savefig('results/param_search_results_linsvm_rocauc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RBF SVM ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy\n",
    "param_search_results_rbfsvm_acc = pd.DataFrame()\n",
    "\n",
    "for i in rbfsvm_acc_idx:\n",
    "    param_search_results = pd.DataFrame(cv_results[i]['params'])\n",
    "    param_search_results['score_acc'] = cv_results[i]['mean_test_score']\n",
    "    param_search_results_rbfsvm_acc = pd.concat([param_search_results_rbfsvm_acc, param_search_results], ignore_index=True)\n",
    "\n",
    "param_search_results_rbfsvm_acc = param_search_results_rbfsvm_acc.groupby(['C', 'gamma'], as_index=True, group_keys=False).mean()\n",
    "param_search_results_rbfsvm_acc.reset_index(inplace=True)\n",
    "sns.heatmap(param_search_results_rbfsvm_acc.pivot('C', 'gamma', 'score_acc'), annot=True, fmt='.4f')\n",
    "plt.xlabel('gamma')\n",
    "plt.title('RBF SVM Accuracy Score')\n",
    "plt.savefig('results/param_search_results_rbfsvm_acc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## F1\n",
    "param_search_results_rbfsvm_f1 = pd.DataFrame()\n",
    "\n",
    "for i in rbfsvm_f1_idx:\n",
    "    param_search_results = pd.DataFrame(cv_results[i]['params'])\n",
    "    param_search_results['score_f1'] = cv_results[i]['mean_test_score']\n",
    "    param_search_results_rbfsvm_f1 = pd.concat([param_search_results_rbfsvm_f1, param_search_results], ignore_index=True)\n",
    "\n",
    "param_search_results_rbfsvm_f1 = param_search_results_rbfsvm_f1.groupby(['C', 'gamma'], as_index=True, group_keys=False).mean()\n",
    "param_search_results_rbfsvm_f1.reset_index(inplace=True)\n",
    "sns.heatmap(param_search_results_rbfsvm_f1.pivot('C', 'gamma', 'score_f1'), annot=True, fmt='.4f')\n",
    "plt.xlabel('gamma')\n",
    "plt.title('RBF SVM F1 Score')\n",
    "plt.savefig('results/param_search_results_rbfsvm_f1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROC AUC\n",
    "param_search_results_rbfsvm_rocauc = pd.DataFrame()\n",
    "\n",
    "for i in rbfsvm_rocauc_idx:\n",
    "    param_search_results = pd.DataFrame(cv_results[i]['params'])\n",
    "    param_search_results['score_rocauc'] = cv_results[i]['mean_test_score']\n",
    "    param_search_results_rbfsvm_rocauc = pd.concat([param_search_results_rbfsvm_rocauc, param_search_results], ignore_index=True)\n",
    "\n",
    "param_search_results_rbfsvm_rocauc = param_search_results_rbfsvm_rocauc.groupby(['C', 'gamma'], as_index=True, group_keys=False).mean()\n",
    "param_search_results_rbfsvm_rocauc.reset_index(inplace=True)\n",
    "sns.heatmap(param_search_results_rbfsvm_rocauc.pivot('C', 'gamma', 'score_rocauc'), annot=True, fmt='.4f')\n",
    "plt.xlabel('gamma')\n",
    "plt.title('RBF SVM ROC AUC Score')\n",
    "plt.savefig('results/param_search_results_rbfsvm_rocauc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy\n",
    "param_search_results_dectre_acc = pd.DataFrame()\n",
    "\n",
    "for i in dectre_acc_idx:\n",
    "    param_search_results = pd.DataFrame(cv_results[i]['params'])\n",
    "    param_search_results['score_acc'] = cv_results[i]['mean_test_score']\n",
    "    param_search_results_dectre_acc = pd.concat([param_search_results_dectre_acc, param_search_results], ignore_index=True)\n",
    "\n",
    "param_search_results_dectre_acc = param_search_results_dectre_acc.groupby(['max_depth', 'ccp_alpha', 'criterion'], as_index=True, group_keys=False).mean()\n",
    "param_search_results_dectre_acc.reset_index(inplace=True)\n",
    "\n",
    "param_search_results_dectre_acc_entropy = param_search_results_dectre_acc[param_search_results_dectre_acc['criterion'] == 'entropy']\n",
    "param_search_results_dectre_acc_entropy = param_search_results_dectre_acc_entropy.drop(columns=['criterion'])\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(param_search_results_dectre_acc_entropy.pivot('max_depth', 'ccp_alpha', 'score_acc'), annot=True, fmt='.4f')\n",
    "plt.xlabel('ccp_alpha')\n",
    "plt.title('Decision Tree (Entropy) Accuracy Score')\n",
    "plt.savefig('results/param_search_results_dectre_entropy_acc.png')\n",
    "plt.show()\n",
    "\n",
    "param_search_results_dectre_acc_gini = param_search_results_dectre_acc[param_search_results_dectre_acc['criterion'] == 'gini']\n",
    "param_search_results_dectre_acc_gini = param_search_results_dectre_acc_gini.drop(columns=['criterion'])\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(param_search_results_dectre_acc_gini.pivot('max_depth', 'ccp_alpha', 'score_acc'), annot=True, fmt='.4f')\n",
    "plt.xlabel('ccp_alpha')\n",
    "plt.title('Decision Tree (Gini) Accuracy Score')\n",
    "plt.savefig('results/param_search_results_dectre_gini_acc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## F1\n",
    "param_search_results_dectre_f1 = pd.DataFrame()\n",
    "\n",
    "for i in dectre_f1_idx:\n",
    "    param_search_results = pd.DataFrame(cv_results[i]['params'])\n",
    "    param_search_results['score_f1'] = cv_results[i]['mean_test_score']\n",
    "    param_search_results_dectre_f1 = pd.concat([param_search_results_dectre_f1, param_search_results], ignore_index=True)\n",
    "\n",
    "param_search_results_dectre_f1 = param_search_results_dectre_f1.groupby(['max_depth', 'ccp_alpha', 'criterion'], as_index=True, group_keys=False).mean()\n",
    "param_search_results_dectre_f1.reset_index(inplace=True)\n",
    "\n",
    "param_search_results_dectre_f1_entropy = param_search_results_dectre_f1[param_search_results_dectre_f1['criterion'] == 'entropy']\n",
    "param_search_results_dectre_f1_entropy = param_search_results_dectre_f1_entropy.drop(columns=['criterion'])\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(param_search_results_dectre_f1_entropy.pivot('max_depth', 'ccp_alpha', 'score_f1'), annot=True, fmt='.4f')\n",
    "plt.xlabel('ccp_alpha')\n",
    "plt.title('Decision Tree (Entropy) F1 Score')\n",
    "plt.savefig('results/param_search_results_dectre_entropy_f1.png')\n",
    "plt.show()\n",
    "\n",
    "param_search_results_dectre_f1_gini = param_search_results_dectre_f1[param_search_results_dectre_f1['criterion'] == 'gini']\n",
    "param_search_results_dectre_f1_gini = param_search_results_dectre_f1_gini.drop(columns=['criterion'])\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(param_search_results_dectre_f1_gini.pivot('max_depth', 'ccp_alpha', 'score_f1'), annot=True, fmt='.4f')\n",
    "plt.xlabel('ccp_alpha')\n",
    "plt.title('Decision Tree (Gini) F1 Score')\n",
    "plt.savefig('results/param_search_results_dectre_gini_f1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROC AUC\n",
    "param_search_results_dectre_rocauc = pd.DataFrame()\n",
    "\n",
    "for i in dectre_rocauc_idx:\n",
    "    param_search_results = pd.DataFrame(cv_results[i]['params'])\n",
    "    param_search_results['score_rocauc'] = cv_results[i]['mean_test_score']\n",
    "    param_search_results_dectre_rocauc = pd.concat([param_search_results_dectre_rocauc, param_search_results], ignore_index=True)\n",
    "\n",
    "param_search_results_dectre_rocauc = param_search_results_dectre_rocauc.groupby(['max_depth', 'ccp_alpha', 'criterion'], as_index=True, group_keys=False).mean()\n",
    "param_search_results_dectre_rocauc.reset_index(inplace=True)\n",
    "\n",
    "param_search_results_dectre_rocauc_entropy = param_search_results_dectre_rocauc[param_search_results_dectre_rocauc['criterion'] == 'entropy']\n",
    "param_search_results_dectre_rocauc_entropy = param_search_results_dectre_rocauc_entropy.drop(columns=['criterion'])\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(param_search_results_dectre_rocauc_entropy.pivot('max_depth', 'ccp_alpha', 'score_rocauc'), annot=True, fmt='.4f')\n",
    "plt.xlabel('ccp_alpha')\n",
    "plt.title('Decision Tree (Entropy) ROC AUC Score')\n",
    "plt.savefig('results/param_search_results_dectre_entropy_rocauc.png')\n",
    "plt.show()\n",
    "\n",
    "param_search_results_dectre_rocauc_gini = param_search_results_dectre_rocauc[param_search_results_dectre_rocauc['criterion'] == 'gini']\n",
    "param_search_results_dectre_rocauc_gini = param_search_results_dectre_rocauc_gini.drop(columns=['criterion'])\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(param_search_results_dectre_rocauc_gini.pivot('max_depth', 'ccp_alpha', 'score_rocauc'), annot=True, fmt='.4f')\n",
    "plt.xlabel('ccp_alpha')\n",
    "plt.title('Decision Tree (Gini) ROC AUC Score')\n",
    "plt.savefig('results/param_search_results_dectre_gini_rocauc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy\n",
    "param_search_results_ranfor_acc = pd.DataFrame()\n",
    "\n",
    "for i in ranfor_acc_idx:\n",
    "    param_search_results = pd.DataFrame(cv_results[i]['params'])\n",
    "    param_search_results['score_acc'] = cv_results[i]['mean_test_score']\n",
    "    param_search_results_ranfor_acc = pd.concat([param_search_results_ranfor_acc, param_search_results], ignore_index=True)\n",
    "\n",
    "param_search_results_ranfor_acc = param_search_results_ranfor_acc.groupby(['n_estimators', 'ccp_alpha', 'criterion'], as_index=True, group_keys=False).mean()\n",
    "param_search_results_ranfor_acc.reset_index(inplace=True)\n",
    "\n",
    "param_search_results_ranfor_acc_entropy = param_search_results_ranfor_acc[param_search_results_ranfor_acc['criterion'] == 'entropy']\n",
    "param_search_results_ranfor_acc_entropy = param_search_results_ranfor_acc_entropy.drop(columns=['criterion'])\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(param_search_results_ranfor_acc_entropy.pivot('n_estimators', 'ccp_alpha', 'score_acc'), annot=True, fmt='.4f')\n",
    "plt.xlabel('ccp_alpha')\n",
    "plt.title('Random Forest (Entropy) Accuracy Score')\n",
    "plt.savefig('results/param_search_results_ranfor_entropy_acc.png')\n",
    "plt.show()\n",
    "\n",
    "param_search_results_ranfor_acc_gini = param_search_results_ranfor_acc[param_search_results_ranfor_acc['criterion'] == 'gini']\n",
    "param_search_results_ranfor_acc_gini = param_search_results_ranfor_acc_gini.drop(columns=['criterion'])\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(param_search_results_ranfor_acc_gini.pivot('n_estimators', 'ccp_alpha', 'score_acc'), annot=True, fmt='.4f')\n",
    "plt.xlabel('ccp_alpha')\n",
    "plt.title('Random Forest (Gini) Accuracy Score')\n",
    "plt.savefig('results/param_search_results_ranfor_gini_acc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## F1\n",
    "param_search_results_ranfor_f1 = pd.DataFrame()\n",
    "\n",
    "for i in ranfor_f1_idx:\n",
    "    param_search_results = pd.DataFrame(cv_results[i]['params'])\n",
    "    param_search_results['score_f1'] = cv_results[i]['mean_test_score']\n",
    "    param_search_results_ranfor_f1 = pd.concat([param_search_results_ranfor_f1, param_search_results], ignore_index=True)\n",
    "\n",
    "param_search_results_ranfor_f1 = param_search_results_ranfor_f1.groupby(['n_estimators', 'ccp_alpha', 'criterion'], as_index=True, group_keys=False).mean()\n",
    "param_search_results_ranfor_f1.reset_index(inplace=True)\n",
    "\n",
    "param_search_results_ranfor_f1_entropy = param_search_results_ranfor_f1[param_search_results_ranfor_f1['criterion'] == 'entropy']\n",
    "param_search_results_ranfor_f1_entropy = param_search_results_ranfor_f1_entropy.drop(columns=['criterion'])\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(param_search_results_ranfor_f1_entropy.pivot('n_estimators', 'ccp_alpha', 'score_f1'), annot=True, fmt='.4f')\n",
    "plt.xlabel('ccp_alpha')\n",
    "plt.title('Random Forest (Entropy) F1 Score')\n",
    "plt.savefig('results/param_search_results_ranfor_entropy_f1.png')\n",
    "plt.show()\n",
    "\n",
    "param_search_results_ranfor_f1_gini = param_search_results_ranfor_f1[param_search_results_ranfor_f1['criterion'] == 'gini']\n",
    "param_search_results_ranfor_f1_gini = param_search_results_ranfor_f1_gini.drop(columns=['criterion'])\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(param_search_results_ranfor_f1_gini.pivot('n_estimators', 'ccp_alpha', 'score_f1'), annot=True, fmt='.4f')\n",
    "plt.xlabel('ccp_alpha')\n",
    "plt.title('Random Forest (Gini) F1 Score')\n",
    "plt.savefig('results/param_search_results_ranfor_gini_f1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROC AUC\n",
    "param_search_results_ranfor_rocauc = pd.DataFrame()\n",
    "\n",
    "for i in ranfor_rocauc_idx:\n",
    "    param_search_results = pd.DataFrame(cv_results[i]['params'])\n",
    "    param_search_results['score_rocauc'] = cv_results[i]['mean_test_score']\n",
    "    param_search_results_ranfor_rocauc = pd.concat([param_search_results_ranfor_rocauc, param_search_results], ignore_index=True)\n",
    "\n",
    "param_search_results_ranfor_rocauc = param_search_results_ranfor_rocauc.groupby(['n_estimators', 'ccp_alpha', 'criterion'], as_index=True, group_keys=False).mean()\n",
    "param_search_results_ranfor_rocauc.reset_index(inplace=True)\n",
    "\n",
    "param_search_results_ranfor_rocauc_entropy = param_search_results_ranfor_rocauc[param_search_results_ranfor_rocauc['criterion'] == 'entropy']\n",
    "param_search_results_ranfor_rocauc_entropy = param_search_results_ranfor_rocauc_entropy.drop(columns=['criterion'])\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(param_search_results_ranfor_rocauc_entropy.pivot('n_estimators', 'ccp_alpha', 'score_rocauc'), annot=True, fmt='.4f')\n",
    "plt.xlabel('ccp_alpha')\n",
    "plt.title('Random Forest (Entropy) ROC AUC Score')\n",
    "plt.savefig('results/param_search_results_ranfor_entropy_rocauc.png')\n",
    "plt.show()\n",
    "\n",
    "param_search_results_ranfor_rocauc_gini = param_search_results_ranfor_rocauc[param_search_results_ranfor_rocauc['criterion'] == 'gini']\n",
    "param_search_results_ranfor_rocauc_gini = param_search_results_ranfor_rocauc_gini.drop(columns=['criterion'])\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(param_search_results_ranfor_rocauc_gini.pivot('n_estimators', 'ccp_alpha', 'score_rocauc'), annot=True, fmt='.4f')\n",
    "plt.xlabel('ccp_alpha')\n",
    "plt.title('Random Forest (Gini) ROC AUC Score')\n",
    "plt.savefig('results/param_search_results_ranfor_gini_rocauc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting Classifier ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accuracy\n",
    "param_search_results_graboo_acc = pd.DataFrame()\n",
    "\n",
    "for i in graboo_acc_idx:\n",
    "    param_search_results = pd.DataFrame(cv_results[i]['params'])\n",
    "    param_search_results['score_acc'] = cv_results[i]['mean_test_score']\n",
    "    param_search_results_graboo_acc = pd.concat([param_search_results_graboo_acc, param_search_results], ignore_index=True)\n",
    "\n",
    "param_search_results_graboo_acc = param_search_results_graboo_acc.groupby(['learning_rate', 'n_estimators'], as_index=True, group_keys=False).mean()\n",
    "param_search_results_graboo_acc.reset_index(inplace=True)\n",
    "sns.heatmap(param_search_results_graboo_acc.pivot('learning_rate', 'n_estimators', 'score_acc'), annot=True, fmt='.4f')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.title('Gradient Boosting Accuracy Score')\n",
    "plt.savefig('results/param_search_results_graboo_acc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## F1\n",
    "param_search_results_graboo_f1 = pd.DataFrame()\n",
    "\n",
    "for i in graboo_f1_idx:\n",
    "    param_search_results = pd.DataFrame(cv_results[i]['params'])\n",
    "    param_search_results['score_f1'] = cv_results[i]['mean_test_score']\n",
    "    param_search_results_graboo_f1 = pd.concat([param_search_results_graboo_f1, param_search_results], ignore_index=True)\n",
    "\n",
    "param_search_results_graboo_f1 = param_search_results_graboo_f1.groupby(['learning_rate', 'n_estimators'], as_index=True, group_keys=False).mean()\n",
    "param_search_results_graboo_f1.reset_index(inplace=True)\n",
    "sns.heatmap(param_search_results_graboo_f1.pivot('learning_rate', 'n_estimators', 'score_f1'), annot=True, fmt='.4f')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.title('Gradient Boosting F1 Score')\n",
    "plt.savefig('results/param_search_results_graboo_f1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROC AUC\n",
    "param_search_results_graboo_rocauc = pd.DataFrame()\n",
    "\n",
    "for i in graboo_rocauc_idx:\n",
    "    param_search_results = pd.DataFrame(cv_results[i]['params'])\n",
    "    param_search_results['score_rocauc'] = cv_results[i]['mean_test_score']\n",
    "    param_search_results_graboo_rocauc = pd.concat([param_search_results_graboo_rocauc, param_search_results], ignore_index=True)\n",
    "\n",
    "param_search_results_graboo_rocauc = param_search_results_graboo_rocauc.groupby(['learning_rate', 'n_estimators'], as_index=True, group_keys=False).mean()\n",
    "param_search_results_graboo_rocauc.reset_index(inplace=True)\n",
    "sns.heatmap(param_search_results_graboo_rocauc.pivot('learning_rate', 'n_estimators', 'score_rocauc'), annot=True, fmt='.4f')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.title('Gradient Boosting ROC AUC Score')\n",
    "plt.savefig('results/param_search_results_graboo_rocauc.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
